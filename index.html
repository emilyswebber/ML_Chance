<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Machine Learning, P-values and Confidence Intervals</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="home.html">Home</a>
</li>
<li>
  <a href="index.html">Report</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Machine Learning, P-values and Confidence Intervals</h1>

</div>


<p> </p>
<div id="how-do-you-know-your-ml-results-arent-due-to-chance" class="section level3">
<h3>How do you know your ML results aren’t due to chance?</h3>
<p>  In hypothesis testing, scientists use p-values and confidence intervals to make sure their observations are not due to chance. Can we do something similar in ML?</p>
<p>In machine learning, data scientists generally look at out-of-sample validation to determine the fitness of a model. But can we test specifically if the model is picking up relationships that are really there, or are only there by chance?</p>
<p>I think this would be valuable in situations where you have very wide datasets with many columns and few samples, or if you are modeling on small datasets generally, for example if you don’t have enough data for a test set.</p>
<p>I used the titanic dataset on kaggle to explore this idea.</p>
<p> </p>
</div>
<div id="the-first-step-is-to-get-the-training-set-and-get-it-ready-for-modeling." class="section level3">
<h3>The first step is to get the training set and get it ready for modeling.</h3>
<pre class="r"><code>#Get Train Set Ready *******
Boat &lt;- read.csv(&quot;train.csv&quot;)

#Title and Last Name
Boat$title&lt;- word(Boat$Name,1, sep = fixed(&#39;.&#39;))
Boat$title&lt;- word(Boat$title,-1)
Boat$lastName &lt;- word(Boat$Name, 1)

##Embarked
Boat$Embarked[Boat$Embarked == &quot;&quot;] &lt;- &quot;C&quot;

##Remove
Boat$Ticket &lt;- NULL
Boat$lastName &lt;- NULL
TrainID &lt;- Boat$PassengerId
Boat$PassengerId &lt;- NULL
Boat$Name &lt;- NULL

##Factors
Boat$Sex &lt;- as.numeric(Boat$Sex)
Boat$Pclass &lt;- as.factor(as.numeric(Boat$Pclass))

##Age
Boat$Age[is.na(Boat$Age)] &lt;- median(Boat$Age, na.rm = TRUE)
Boat$Fare[is.na(Boat$Fare)] &lt;- median(Boat$Fare, na.rm = TRUE)

##Title
Boat$title2[Boat$title == &quot;Lady&quot; |
              Boat$title == &quot;Countess&quot; |
              Boat$title == &quot;Don&quot; |
              Boat$title == &quot;Dr&quot; |
              Boat$title == &quot;Rev&quot; |
              Boat$title == &quot;Sir&quot; |
              Boat$title == &quot;Johnkheer&quot;] &lt;- &quot;Rare&quot;
            
Boat$title2[Boat$title == &quot;Major&quot; |
               Boat$title == &quot;Countess&quot; |
               Boat$title == &quot;Capt&quot;] &lt;- &quot;Officer&quot;


Boat$title2[is.na(Boat$title2)] &lt;- &quot;Common&quot;
Boat$title2 &lt;- NULL            
Boat$Cabin &lt;- NULL      
#write.csv(Boat, file = &quot;trainboat_fin.csv&quot;)</code></pre>
<p> </p>
</div>
<div id="i-ran-an-xbgtree-algorithm-on-the-training-data." class="section level3">
<h3>I ran an xbgTree algorithm on the training data.</h3>
<pre class="r"><code>#Rmodel*****************

set.seed(1)

xbg &lt;- train(factor(Survived) ~.,
             method = &#39;xgbTree&#39;,
             data = Boat); print(xbg)</code></pre>
<p> </p>
</div>
<div id="the-confusion-matrix-shows-the-model-has-an-overall-accuracy-83." class="section level3">
<h3>The confusion matrix shows the model has an overall accuracy ~ 83%.</h3>
<pre class="r"><code>confusionMatrix(xbg)</code></pre>
<pre><code>FALSE Bootstrapped (25 reps) Confusion Matrix 
FALSE 
FALSE (entries are percentual average cell counts across resamples)
FALSE  
FALSE           Reference
FALSE Prediction    0    1
FALSE          0 54.8 10.1
FALSE          1  7.1 28.0
FALSE                            
FALSE  Accuracy (average) : 0.828</code></pre>
<p> </p>
</div>
<div id="this-is-where-it-gets-interesting" class="section level3">
<h3>This is where it get’s interesting…</h3>
<p>So I have a basic model that I ran on a famous dataset, now what?</p>
<p>I want to be able to examine whether or not the modeling is accurate <em>by chance</em>. On approach that comes to mind is to randomize the dataset and rerun it through the algorithm.</p>
<pre class="r"><code>##Perturb Data *****
Boat_random &lt;- Boat
Boat_Label &lt;- Boat$Survived
Boat_random$Survived &lt;- NULL
Boat_random = as.data.frame(lapply(Boat_random, function(x) { sample(x) }))</code></pre>
<p> </p>
</div>
<div id="run-the-randomized-dataset-through-the-original-model." class="section level3">
<h3>Run the randomized dataset through the original model.</h3>
<p>The data in this case is randomized, meaning that the columns do not contain information regarding the target. The model performance should degrade in response this randomization.</p>
<pre class="r"><code>random_pred &lt;-predict(xbg, Boat_random, type=&quot;prob&quot;)
random_pred2 &lt;-predict(xbg, Boat_random)</code></pre>
<p> </p>
</div>
<div id="lets-take-a-look-at-the-confusion-matrix." class="section level3">
<h3>Let’s take a look at the confusion matrix.</h3>
<p>The confusion matrix shows an accuracy of ~53%</p>
<pre class="r"><code>random_pred2 &lt;- as.factor(random_pred2)
Actual &lt;- as.factor(Boat_Label)
confusionMatrix(Actual, random_pred2)</code></pre>
<pre><code>FALSE Confusion Matrix and Statistics
FALSE 
FALSE           Reference
FALSE Prediction   0   1
FALSE          0 367 182
FALSE          1 232 110
FALSE                                          
FALSE                Accuracy : 0.5354         
FALSE                  95% CI : (0.502, 0.5685)
FALSE     No Information Rate : 0.6723         
FALSE     P-Value [Acc &gt; NIR] : 1.00000        
FALSE                                          
FALSE                   Kappa : -0.0102        
FALSE                                          
FALSE  Mcnemar&#39;s Test P-Value : 0.01603        
FALSE                                          
FALSE             Sensitivity : 0.6127         
FALSE             Specificity : 0.3767         
FALSE          Pos Pred Value : 0.6685         
FALSE          Neg Pred Value : 0.3216         
FALSE              Prevalence : 0.6723         
FALSE          Detection Rate : 0.4119         
FALSE    Detection Prevalence : 0.6162         
FALSE       Balanced Accuracy : 0.4947         
FALSE                                          
FALSE        &#39;Positive&#39; Class : 0              
FALSE </code></pre>
<p> </p>
</div>
<div id="next-i-wanted-to-see-the-error-distribution-of-the-random-model." class="section level3">
<h3>Next I wanted to see the error distribution of the random model.</h3>
<p> </p>
<p>The model shouldn’t predict well because there are no real relationships in the data. However, it should get some right just by chance. I used a simple difference metric to evaluate the fitness of the model. Specifically, I subtracted the predicted probability by the actual value (0 or 1). I then took the median of the absolute difference.</p>
<pre class="r"><code>Boat_random$Actual &lt;- Boat_Label
Boat_random$Random_Pred &lt;- random_pred[,2]

Boat_random$Survived[Boat_random$Actual == 1] &lt;- &quot;Yes&quot;
Boat_random$Survived[Boat_random$Actual == 0] &lt;- &quot;No&quot;

Boat_random$diff &lt;- Boat_random$Actual - Boat_random$Random_Pred
Boat_random$diff_abs &lt;- abs(Boat_random$diff)
median(Boat_random$diff_abs)</code></pre>
<pre><code>FALSE [1] 0.4612427</code></pre>
<p> </p>
</div>
<div id="with-random-data-the-median-error-of-the-model-is-46." class="section level3">
<h3>With random data the median error of the model is ~46%.</h3>
<p> </p>
<p>You can see above that the algorithm did not predict well for either class when the data was randomized. <img src="index_files/figure-html/unnamed-chunk-8-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p> </p>
</div>
<div id="in-order-compare-apples-to-apples-i-ran-the-training-data-through-the-model-again-as-well." class="section level3">
<h3>In order compare apples to apples, I ran the training data through the model again as well.</h3>
<p> </p>
<p>With meaningful data the median error of the model is only ~11%. This is a large reduction of error compared to model that predicted on random data. This is a good sign that our model is picking up relevant patterns.</p>
<pre class="r"><code>#Predict on training data for comparison**********
boat_training_exp &lt;- Boat
survived &lt;- boat_training_exp$Survived 
boat_training_exp$Survived &lt;- NULL

real_pred &lt;-predict(xbg, boat_training_exp,type=&quot;prob&quot;)

boat_training_exp$prediction &lt;- real_pred[,2]
boat_training_exp$actual &lt;- survived

boat_training_exp$diff &lt;- boat_training_exp$actual - boat_training_exp$prediction

boat_training_exp$diff_abs &lt;- abs(boat_training_exp$diff)
median(boat_training_exp$diff_abs)</code></pre>
<p> </p>
<p><img src="index_files/figure-html/unnamed-chunk-10-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p> </p>
</div>
<div id="there-is-a-reduction-in-error-of-the-predictions-when-i-add-meaningful-vs.random-data.-but-this-doesnt-address-whether-or-not-this-decrease-is-due-to-chance" class="section level3">
<h3>There is a reduction in error of the predictions when I add meaningful vs. random data. But this doesn’t address whether or not this decrease is <em>due to chance</em></h3>
<p> </p>
<p>I decided can borrow a concept from non-parametric statistics and use a wilcoxon-signed-rank test to compare the distributions of absolute errors (random vs. training). I chose this test because it relies on medians rather than means, so it does not assume a normal distribution.</p>
<pre class="r"><code>wilcoxon_data = data.frame(Random= Boat_random$diff_abs, Training=boat_training_exp$diff_abs) 

res &lt;-wilcox.test(wilcoxon_data$Training, wilcoxon_data$Random)
res</code></pre>
<pre><code>FALSE 
FALSE   Wilcoxon rank sum test with continuity correction
FALSE 
FALSE data:  wilcoxon_data$Training and wilcoxon_data$Random
FALSE W = 189736, p-value &lt; 0.00000000000000022
FALSE alternative hypothesis: true location shift is not equal to 0</code></pre>
<p> </p>
<p>The results show that the p-value is very small (0.00000000000000000000), meaning that we can with reasonable credibility assert that the reduction in error that we see with our XBGTree model predicting on real vs. random data is likely not due to chance.</p>
<p> </p>
</div>
<div id="what-about-confidence-intervals" class="section level3">
<h3>What about confidence intervals?</h3>
<p> </p>
<p>P-values are nice, but they are not especially useful on their own. Confidence intervals would be a big boost to my confidence in the differences.</p>
<p>I used another non-parametric approach to calculate these as well. Specifically, I used a bootstrapping technique and took 1000 random samples from the random and training absolute error and calculated the median for each of those random samples.</p>
<pre class="r"><code>bootstrap_data = data.frame(Random= Boat_random$diff_abs, Training=boat_training_exp$diff_abs)

med.diff2 &lt;- function(data, indices) {
  d &lt;- data[indices] # allows boot to select sample 
    return(median(d))
} 

# bootstrapping with 1000 replications 
results_training &lt;- boot(data=bootstrap_data$Training, statistic=med.diff2, R=1000)

results_random &lt;- boot(data=bootstrap_data$Random, statistic=med.diff2, R=1000)
#kable(results_random)</code></pre>
<p> </p>
</div>
<div id="results-for-the-training-dataset" class="section level3">
<h3>Results for the Training dataset</h3>
<p> </p>
<p>Results for the non-random training dataset showed that 95% of the medians calculated from 1000 bootstrapped error samples are between ~10% and ~12%.</p>
<p><img src="index_files/figure-html/unnamed-chunk-13-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p> </p>
</div>
<div id="confidence-intervals-for-training-dataset" class="section level3">
<h3>Confidence intervals for Training dataset</h3>
<pre class="r"><code># get 95% confidence interval 
tci &lt;- boot.ci(results_training, type=c(&quot;norm&quot;))</code></pre>
<p>confidence interval is: 95.0%, 10.3%, 12.1%</p>
<p> </p>
</div>
<div id="results-for-the-random-dataset" class="section level3">
<h3>Results for the Random dataset</h3>
<p> </p>
<p>Results for the random dataset showed that 95% of the medians calculated from 1000 bootstrapped error samples are between ~42% to ~52%.</p>
<p><img src="index_files/figure-html/unnamed-chunk-15-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p> </p>
</div>
<div id="confidence-intervals-for-random-dataset" class="section level3">
<h3>Confidence intervals for Random dataset</h3>
<pre class="r"><code># get 95% confidence interval 
rci &lt;-boot.ci(results_random, type=c(&quot;norm&quot;))</code></pre>
<p> </p>
<p>confidence interval is: 95.0%, 41.6%, 51.8%</p>
<p> </p>
</div>
<div id="does-this-difference-in-these-metrics-decrease-with-smaller-samples-sizes" class="section level3">
<h3>Does this difference in these metrics decrease with smaller samples sizes?</h3>
<p>I want to see if these metrics degrade with smaller sample sizes. If so, then this logic may really be a good measure of whether or not your modeling results are due to chance. With smaller datasets one should be less confident.</p>
<p> </p>
</div>
<div id="p-value" class="section level3">
<h3>P-value</h3>
<p>You can see below that the p-value for the smaller dataset is larger than for all of the data, suggesting that this logic scales.</p>
<pre class="r"><code># get 95% confidence interval 

wilcoxon_data_10per = sample_n(wilcoxon_data, 89)

res_2 &lt;-wilcox.test(wilcoxon_data_10per$Training, wilcoxon_data_10per$Random)
res_2</code></pre>
<pre><code>FALSE 
FALSE   Wilcoxon rank sum test with continuity correction
FALSE 
FALSE data:  wilcoxon_data_10per$Training and wilcoxon_data_10per$Random
FALSE W = 2200, p-value = 0.0000003052
FALSE alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Whole datset p-value is: 0</p>
<p> </p>
<p>10% datset p-value is: 0.0000003</p>
<p> </p>
</div>
<div id="bootstrapping" class="section level3">
<h3>Bootstrapping</h3>
<p>I applied the same bootstrapping principles to the small dataset as well.</p>
<pre class="r"><code>small_data &lt;- boat_training_exp
colnames(small_data)[colnames(small_data)==&quot;diff_abs&quot;] &lt;- &quot;train_diff&quot;
small_data$random_diff &lt;- Boat_random$diff_abs

small_data &lt;- sample_n(small_data, 89)

# bootstrapping with 1000 replications 
results_training_small &lt;- boot(data=small_data$train_diff, statistic=med.diff2, R=1000)

results_random_small &lt;- boot(data=small_data$random_diff, statistic=med.diff2, R=1000)</code></pre>
<p> </p>
</div>
<div id="non-random-training-data-10" class="section level3">
<h3>Non-random Training data: 10%</h3>
<p>The confidence interals for the small vs. whole dataset are wider for the non-randomized sample.</p>
<p><img src="index_files/figure-html/unnamed-chunk-19-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p> </p>
</div>
<div id="confidence-intervals-for-training-data-10" class="section level3">
<h3>Confidence intervals for training data: 10%</h3>
<pre class="r"><code># get 95% confidence interval 
tci2 &lt;-boot.ci(results_training_small, type=c(&quot;norm&quot;))
tci2$normal</code></pre>
<pre><code>FALSE      conf                     
FALSE [1,] 0.95 0.06630774 0.1403901</code></pre>
<p> </p>
<p>Whole datset confidence interval is: 95.0%, 10.3%, 12.1%</p>
<p> </p>
<p>10% datset confidence interval is: 95.0%, 6.6%, 14.0%</p>
<p> </p>
</div>
<div id="randomized-data-10" class="section level3">
<h3>Randomized data: 10%</h3>
<p>The confidence interals for the small vs. whole dataset are wider for the random sample as well.</p>
<p><img src="index_files/figure-html/unnamed-chunk-21-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p> </p>
</div>
<div id="confidence-intervals-for-random-dataset-10" class="section level3">
<h3>Confidence intervals for random dataset: 10%</h3>
<pre class="r"><code># get 95% confidence interval 
rci2 &lt;-boot.ci(results_random_small, type=c(&quot;norm&quot;))
rci2$normal</code></pre>
<pre><code>FALSE      conf                    
FALSE [1,] 0.95 0.2518102 0.5783673</code></pre>
<p> </p>
<p>Whole datset confidence interval is: 95.0%, 41.6%, 51.8%</p>
<p> </p>
<p>10% datset confidence interval is: 95.0%, 25.2%, 57.8%</p>
<p> </p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<ol style="list-style-type: decimal">
<li><p>I created an XGBTree model with the caret package on the Titanic dataset with about ~83% accuracy.</p></li>
<li><p>When I ran a randomized dataset through the model the accuracy dropped by quite a lot to ~53%.</p></li>
<li><p>I calculated the differences between the predicted and actual values simply by subtracting the probability of surival by the class (0 or 1) and taking the absolute value. I then calculated the medians.</p></li>
</ol>
<p>The median errors for the random data: 46.1%</p>
<p>The median errors for the non-random data: 11.3%</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Then I plotted the distributions of these errors for the random and non-random samples.</p></li>
<li><p>I then wanted to see if the differences were <em>due to chance</em>, so I rand those error’s through a Wilcoxon sign ranked test - which is a way to compare to non-normal distributions. This test determined that these error distributions are significantly different: p-value: 0.00000000000000000000.</p></li>
<li><p>After finding the p-vlaue I used a non-parametric bootstrapping method to calculate confidence intervals.</p></li>
</ol>
<p>The confidence intervals in the non-random data (95.0%, 10.3%, 12.1%) are lower than the random data (95.0%, 41.6%, 51.8%), further supporting that these two distributon of errors do not have much overlap.</p>
<ol start="7" style="list-style-type: decimal">
<li>The final thing I wanted to test was whether or not this method would be impacted by sample size. Confidence in findings should theoretically decrease when I use smaller samples, so I took a random sample of 10% of the original dat and recalculated these metrics.</li>
</ol>
<p><strong>P-values</strong></p>
<p>Whole datset p-value is: 0.00000000000000000000</p>
<p> </p>
<p>10% datset p-value is: 0.0000003</p>
<p> </p>
<p><strong>Confidence Intervals: non-random</strong></p>
<p>Whole datset confidence interval for the non-random data is: 95.0%, 10.3%, 12.1%</p>
<p> </p>
<p>10% datset confidence interval for the non-random data is is: 95.0%, 6.6%, 14.0%</p>
<p> </p>
<p><strong>Confidence Intervals: random</strong></p>
<p>Whole datset confidence interval for the random dataset is: 95.0%, 41.6%, 51.8%</p>
<p> </p>
<p>10% datset confidence interval for the random dataset is: 95.0%, 25.2%, 57.8%</p>
<p> </p>
</div>
<div id="discussion" class="section level3">
<h3>Discussion</h3>
<p>I set out to try and apply some of the validation metrics used in statistics to machine learning. The idea is certainly not totally there, but I think this process is a good start to marrying two fundamental concepts between statistics and machine learning.</p>
<p>The p-values and confidence intervals scale down in a way that makes sense with a smaller dataset.</p>
<p>Some other ideas:</p>
<ol style="list-style-type: decimal">
<li><p>Instead of using an absolute error, this could be refined by doing the same process with a metric like LogLoss - which <a href="https://emilyswebber.github.io/LogLoss/">has some interesting properties on it’s own</a>.</p></li>
<li><p>Using non-parametric comparisons make sense if you don’t know the distribution of the data, but what if you do?</p></li>
<li><p>Maybe running one random permutation isn’t enough, are there concepts that can be borrowed from Montecarlo simulations? Or is this just extra work without any benefit?</p></li>
<li><p>I ran a random sample as well as the training sample through the dataset, but I did not use the test sample (mostly because the results are not publically available).</p></li>
</ol>


<link rel="stylesheet" href="style.css" type="text/css" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-91308049-1', 'auto');
  ga('send', 'pageview');

</script>



<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-589036a8549be1ce"></script>



</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
