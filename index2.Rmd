---
title: "Machine Learning, P-values and Confidence Intervals"
output: html_document
---

```{r setup, include=FALSE, results="hide"}

directory <- "/Users/emily.webber/Dropbox/Website Dropbox 2/Chance"
setwd(directory)



library(dplyr)
library(stringr)
library(data.table)
library(caret)
library(e1071)
library(MLmetrics)
library(boot)
```

&nbsp; 

### How do you know your ML results aren't due to chance?

&nbsp; 

In hypothesis testing, scientists use p-values and confidence intervals to make sure their observations are not due to chance.  Can we do something similar in ML?  


I used the titanic dataset on kaggle to explore this idea. 

&nbsp; 

### The first step is to get the training set and get it ready for modeling.
```{r comment=FALSE, results="hide"}

#Get Train Set Ready *******
Boat <- read.csv("train.csv")

#Title and Last Name
Boat$title<- word(Boat$Name,1, sep = fixed('.'))
Boat$title<- word(Boat$title,-1)
Boat$lastName <- word(Boat$Name, 1)

##Embarked
Boat$Embarked[Boat$Embarked == ""] <- "C"

##Remove
Boat$Ticket <- NULL
Boat$lastName <- NULL
TrainID <- Boat$PassengerId
Boat$PassengerId <- NULL
Boat$Name <- NULL

##Factors
Boat$Sex <- as.numeric(Boat$Sex)
Boat$Pclass <- as.factor(as.numeric(Boat$Pclass))

##Age
Boat$Age[is.na(Boat$Age)] <- median(Boat$Age, na.rm = TRUE)
Boat$Fare[is.na(Boat$Fare)] <- median(Boat$Fare, na.rm = TRUE)

##Title
Boat$title2[Boat$title == "Lady" |
              Boat$title == "Countess" |
              Boat$title == "Don" |
              Boat$title == "Dr" |
              Boat$title == "Rev" |
              Boat$title == "Sir" |
              Boat$title == "Johnkheer"] <- "Rare"
            
Boat$title2[Boat$title == "Major" |
               Boat$title == "Countess" |
               Boat$title == "Capt"] <- "Officer"


Boat$title2[is.na(Boat$title2)] <- "Common"
Boat$title2 <- NULL            
Boat$Cabin <- NULL      
#write.csv(Boat, file = "trainboat_fin.csv")

```

&nbsp; 

### I ran an xbgTree algorithm on the training data.

```{r comment=FALSE, results="hide"}
#Rmodel*****************

set.seed(1)

xbg <- train(factor(Survived) ~.,
             method = 'xgbTree',
             data = Boat); print(xbg)
```

&nbsp; 

```{r comment=FALSE}
confusionMatrix(xbg)
```

&nbsp; 

### This is where it get's interesting...

So I have a basic model that I ran on a famous dataset, now what?  

I want to be able to examine whether or not the modeling is accurate *by chance*.  On approach that comes to mind is to randomized the dataset and rerun it through the algorithm. 
```{r comment=FALSE, results="hide"}
##Perturb Data *****
Boat_random <- Boat
Boat_Label <- Boat$Survived
Boat_random$Survived <- NULL
Boat_random = as.data.frame(lapply(Boat_random, function(x) { sample(x) }))
```

&nbsp; 

### The next step is to take the random data use the original model to make predictions.  

The data in this case is randomized, meaning that the columns do not contain information regarding the target.  The model performance should degrade in response this randomization.
```{r comment=FALSE, results="hide"}
random_pred <-predict(xbg, Boat_random, type="prob")
random_pred2 <-predict(xbg, Boat_random)
```

&nbsp; 

### Let's take a look at the confusion matrix. 

The confusion matrix shows an accuracy of ~53%

```{r comment=FALSE}
random_pred2 <- as.factor(random_pred2)
Actual <- as.factor(Boat_Label)
confusionMatrix(Actual, random_pred2)
```

&nbsp; 

### After that, I wanted to see how well the model predicted when there was no pattern in the data in comparison to the actual values. 

&nbsp; 

The model shouldn't predict well because there are no real relationships in the data. However, it should get some right just by chance.  The way I did this was I subtracted the predicted probability by the actual value (0 or 1) and then took the median of the absolute difference).

```{r comment=FALSE}
Boat_random$Actual <- Boat_Label
Boat_random$Random_Pred <- random_pred[,2]

Boat_random$Survived[Boat_random$Actual == 1] <- "Yes"
Boat_random$Survived[Boat_random$Actual == 0] <- "No"

Boat_random$diff <- Boat_random$Actual - Boat_random$Random_Pred
Boat_random$diff_abs <- abs(Boat_random$diff)
median(Boat_random$diff_abs)
```

&nbsp; 

### With random data the model is, as measured by the median, wrong ~46% of the time and correct ~54% of the time.

&nbsp; 

You can see above that the algorithm did not predict well for either class when the data was randomized.
```{r fig.width= 12,  fig.height = 5, echo = FALSE, warning = FALSE, fig.show='hold', fig.align='center', comment=FALSE}

ggplot(aes(x=diff_abs, fill = Survived), data=Boat_random, stat_bin(25)) + 
  geom_histogram(bins = 25) +   
  scale_fill_manual(values = c("dark gray","dark blue")) +
  #ggtitle("Diff for Random Data")  + 
   ylab("Count") + 
   xlab('Absolute Difference') + theme(legend.position="top")
```


&nbsp; 

### I also wanted to see what the this metric would look like for the real training data. 

&nbsp; 

With meaningful data the modeling was incorrect, as measured by the median, only ~11% of the time. This is a large reduction of error compared to model that predicted on random data. 

```{r comment=FALSE, results="hide"}
#Predict on training data for comparison**********
boat_training_exp <- Boat
survived <- boat_training_exp$Survived 
boat_training_exp$Survived <- NULL

real_pred <-predict(xbg, boat_training_exp,type="prob")

boat_training_exp$prediction <- real_pred[,2]
boat_training_exp$actual <- survived

boat_training_exp$diff <- boat_training_exp$actual - boat_training_exp$prediction

boat_training_exp$diff_abs <- abs(boat_training_exp$diff)
median(boat_training_exp$diff_abs)
```

&nbsp; 

```{r fig.width= 12,  fig.height = 5, echo = FALSE, warning = FALSE, fig.show='hold', fig.align='center', comment=FALSE}

boat_training_exp$Survived[boat_training_exp$actual == 1] <- "Yes"
boat_training_exp$Survived[boat_training_exp$actual == 0] <- "No"

ggplot(aes(x=diff_abs, fill = Survived), data=boat_training_exp, stat_bin(25)) + 
  geom_histogram(bins = 25) +   
  scale_fill_manual(values = c("dark gray","dark blue")) +
 # ggtitle("Diff for Training Data")  + 
   ylab("Count") + 
   xlab('Absolute Difference') + theme(legend.position="top")
```

&nbsp; 

### There is a reduction in error of the predictions when I add meaningful vs. random data. But this doesn't address whether or not this decrease is *due to chance*

&nbsp; 

I decided can borrow a concept from non-parametric statistics and use a wilcoxon-signed-rank test to compare the distributions of absolute errors (random vs. training).  I chose this test because it relies on medians rather than means, so it does not assume a normal distribution.

```{r comment=FALSE}
wilcoxon_data = data.frame(Random= Boat_random$diff_abs, Training=boat_training_exp$diff_abs) 

res <-wilcox.test(wilcoxon_data$Training, wilcoxon_data$Random, conf.level = 0.95)
res
```

&nbsp; 

The results show that the p-value is very small, meaning that we can with reasonable credibility assert that the reduction in error that we see with our XBGTree model predicting on real vs. random data is likely not due to chance.

&nbsp; 

### What about confidence intervals? 

P-values are nice, but they are not especially useful on their own. Confidence intervals would be a big boost to my confidence in the differences.

I used another non-parametric approach to calculate these as well. Specifically, I used a bootstrapping technique and took 1000 random samples from the random and training absolute error and calculated the median for each of those random samples.

```{r comment=FALSE, results="hide"}

bootstrap_data = data.frame(Random= Boat_random$diff_abs, Training=boat_training_exp$diff_abs)

med.diff2 <- function(data, indices) {
  d <- data[indices] # allows boot to select sample 
    return(median(d))
} 

# bootstrapping with 1000 replications 
results_training <- boot(data=bootstrap_data$Training, statistic=med.diff2, R=1000)

results_random <- boot(data=bootstrap_data$Random, statistic=med.diff2, R=1000)
#kable(results_random)
```

&nbsp; 

### Results for the Training dataset

&nbsp; 

Results for the non-random training dataset showed that 95% of the medians calculated from 1000 bootstrapped error samples are between ~10% and ~12%. 

```{r fig.width= 12,  fig.height = 5, echo = FALSE, warning = FALSE, fig.show='hold', fig.align='center', comment=FALSE}
plot(results_training)
```

&nbsp; 

### Confidence intervals for Training dataset
```{r comment=FALSE}
# get 95% confidence interval 
tci <- boot.ci(results_training, type=c("norm"))
tci$normal
```

&nbsp; 

### Results for the Random dataset

&nbsp; 

Results for the random dataset showed that 95% of the medians calculated from 1000 bootstrapped error samples are between ~42% to ~52%. 

```{r fig.width= 12,  fig.height = 5, echo = FALSE, warning = FALSE, fig.show='hold', fig.align='center', comment=FALSE}

plot(results_random)
     
```

&nbsp; 

### Confidence intervals for Random dataset
```{r comment=FALSE}
# get 95% confidence interval 
rci <-boot.ci(results_random, type=c("norm"))
rci$normal
```


### Discussion

I set out to try and apply some of the validation metrics used in statistics to machine learning.  The idea certainly not totally there, but I think this process is a good start to marrying two fundamental concepts between statistics and machine learning.

Some other ideas: 

1. Instead of using an absolute error, this could be refined by doing the same process with a metric like LogLoss - which [has some interesting properties on it's own](https://emilyswebber.github.io/LogLoss/).

2. Using non-parametric comparisons make sense if you don't know the distribution of the data, but what if you do?  Is there a pretest that can be applied to determine the distribution of the errors?

3. Maybe running one random permutation isn't enough, are there concepts that can be borrowed from Montecarlo simulations?  Or is this just extra work without any benefit?


<!--html_preserve-->

<link rel="stylesheet" href="style.css" type="text/css" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-91308049-1', 'auto');
  ga('send', 'pageview');

</script>



<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-589036a8549be1ce"></script>


<!--/html_preserve-->


